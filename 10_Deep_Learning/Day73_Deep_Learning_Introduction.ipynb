{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e05a7f6-0297-4fca-a12a-acd45a8c7dba",
   "metadata": {},
   "source": [
    "# Deep Learning – Introduction\n",
    "\n",
    "Welcome to my Deep Learning documentation!  \n",
    "These notes are based on my classroom learning, with additional explanations, examples, and illustrations to make concepts clearer.  \n",
    "\n",
    "**The goal is to:**\n",
    "\n",
    "- Build a solid understanding of the fundamentals of Deep Learning.  \n",
    "- Document each concept in a structured way for future reference.  \n",
    "- Share knowledge with peers and the community.  \n",
    "\n",
    "This notebook covers the basics of neural networks, forward & backward propagation, epochs, and activation functions.  \n",
    "\n",
    "In the upcoming days, I’ll keep adding **ANN, CNN, RNN**, and other deep learning concepts with examples.\n",
    "\n",
    "## What is Deep Learning?\n",
    "Deep Learning (DL) is a subset of **Machine Learning (ML)** where we use **neural network architectures** with multiple layers to automatically learn features from data.\n",
    "- ML algorithms require feature engineering (manually extracting features).\n",
    "- DL algorithms learn these features automatically from raw data (numbers, images, audio, text).\n",
    "\n",
    "## Why Deep Learning?\n",
    "\n",
    "- Handles **complex data** like images, videos, speech, and text.\n",
    "- Learns hierarchical features:\n",
    "    - Example: In image recognition, first layers detect edges, next detect shapes, final layers detect objects.\n",
    "- Powers modern applications like **speech recognition, translation, self-driving cars, and recommendation systems.**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Works well with **large datasets.**\n",
    "- Reduces the need for **manual feature engineering.**\n",
    "- Excellent at handling **unstructured data** (image, video, audio, text).\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Requires **a lot of data.**\n",
    "- Needs **high computational power (GPUs/TPUs).**\n",
    "- Difficult to **interpret (black box models).**\n",
    "- Training can take a **long time.**\n",
    "\n",
    "# Types of Neural Networks\n",
    "\n",
    " Deep learning is built on neural networks. Depending on data type:\n",
    "\n",
    "- **ANN (Artificial Neural Network)** → For structured numerical data.\n",
    "- **CNN (Convolutional Neural Network)** → For images and videos.\n",
    "- **RNN (Recurrent Neural Network)** → For sequential data like text or time series.\n",
    "\n",
    " All three are **Neural Network Architectures (NNA).**\n",
    "\n",
    "# Neural Network Architecture (NNA)\n",
    "\n",
    "Neural networks are inspired by the **Human Neural Network (HNN).**\n",
    "\n",
    "**Example: Mosquito bite**\n",
    "\n",
    "Imagine a mosquito bites your left hand:\n",
    "\n",
    "- **Input layer:** Eyes/skin sense the mosquito.\n",
    "- **Activation function:** Neurons activated.\n",
    "- **Hidden layer:** Brain processes the signal.\n",
    "- **Output layer:** Right hand moves to kill the mosquito.\n",
    "\n",
    "This is similar to **ANN:**\n",
    "\n",
    "- **Input Layer → Hidden Layer(s) → Output Layer.**\n",
    "\n",
    "**General structure:**\n",
    "---\n",
    "![ANN Architecture](ANN_Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23433e12-a70c-4fd0-8cba-9a53c8aa7885",
   "metadata": {},
   "source": [
    "# Concepts We Learned Today\n",
    "\n",
    "##  Neuron\n",
    "\n",
    "- The basic unit of a neural network.  \n",
    "- Receives inputs, applies **weights**, passes through an **activation function**, and gives an output.  \n",
    "\n",
    "**Mathematical representation:**\n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "- **One neuron connected to another neuron** forms a **Neural Network**.  \n",
    "- The strength of the connection is represented by **weights**.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "- The simplest form of a neuron.  \n",
    "- Makes decisions by combining inputs with weights.  \n",
    "- The **signal passing between neurons is called a synapse**.\n",
    "\n",
    "## Layers\n",
    "1. **Input Layer (IP):**\n",
    "\n",
    "   - Takes the raw features.  \n",
    "   - Example: `play (x1), study (x2), sleep (x3)`.  \n",
    "\n",
    "2. **Hidden Layer:**\n",
    "\n",
    "   - Processes inputs with weights and activation functions.  \n",
    "   - Example:  \n",
    "     $$\n",
    "     h = x_1w_1 + x_2w_2 + x_3w_3\n",
    "     $$  \n",
    "\n",
    "3. **Output Layer (OP):**\n",
    "\n",
    "   - Produces final prediction.\n",
    "  \n",
    "## Forward Propagation (FP)\n",
    "\n",
    "- Data flows from **Input → Hidden → Output**.  \n",
    "- We initially assign **random weights**.  \n",
    "- **Example:** Student’s activities (play, study, sleep) are inputs → hidden layer computes → output predicts performance.  \n",
    "\n",
    "## Backward Propagation (BP)\n",
    "\n",
    "- If prediction is wrong, the model **goes back** and adjusts weights.  \n",
    "- The process of updating weights to reduce error is called **backpropagation**.  \n",
    "- Goal: Minimize the difference between **actual vs predicted output**.  \n",
    "\n",
    "## Epoch\n",
    "- **1 Epoch = 1 Forward Propagation (FP) + 1 Backward Propagation (BP)**.  \n",
    "- Multiple epochs are used to improve accuracy until the model converges.  \n",
    "\n",
    "## Activation Functions\n",
    "Activation functions decide whether a neuron should activate (fire) or not.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0be53-72f2-4c87-a7bf-b95d16531155",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activation Functions in Neural Networks\n",
    "\n",
    "## What is an Activation Function?\n",
    "An **activation function** decides whether a neuron should \"fire\" (activate) or not.  \n",
    "\n",
    "- Without activation functions → Neural networks become just **linear functions** (like linear regression).  \n",
    "- With activation functions → Networks can learn **non-linear patterns** in data (like curves, images, language).  \n",
    "\n",
    "**In simple words:** Activation functions introduce **non-linearity**, which allows deep learning models to solve complex problems.  \n",
    "\n",
    "\n",
    "## Why Do We Need Activation Functions?\n",
    "1. **Non-linearity:** Real-world data is complex (e.g., image recognition, speech). Linear functions cannot model them.  \n",
    "2. **Feature learning:** Helps extract advanced features at different layers.  \n",
    "3. **Flexibility:** Different activation functions work better for different tasks (binary classification, multiclass, hidden layers).  \n",
    "\n",
    "\n",
    "# Types of Activation Functions\n",
    "\n",
    "## **Sigmoid Function**\n",
    "\n",
    "- Formula:  \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "- Range: (0, 1).  \n",
    "- Smooth \"S\" shaped curve.  \n",
    "\n",
    "**Where to use:**  \n",
    "- **Binary classification problems** (output = probability).  \n",
    "- Always used in **output layer** when prediction must be probability between 0 and 1.  \n",
    "\n",
    "**Example:**  \n",
    "Predicting if an email is spam (1) or not spam (0).  \n",
    "\n",
    "## **Tanh (Hyperbolic Tangent) Function**\n",
    "\n",
    "- Formula:  \n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "- Range: (-1, 1).  \n",
    "- \"S\" shaped curve, but allows negative values.  \n",
    "\n",
    "**Where to use:**  \n",
    "- When the data can have both positive and negative outputs.  \n",
    "- Better than sigmoid in hidden layers because it is centered around 0.  \n",
    "\n",
    "**Example:**  \n",
    "Sentiment analysis (negative sentiment = -1, positive sentiment = +1).  \n",
    "\n",
    "## **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "- Formula:  \n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "- Range: [0, ∞).  \n",
    "- Very fast and simple.  \n",
    "\n",
    "**Where to use:**  \n",
    "- Most common choice for **hidden layers** in deep learning.  \n",
    "- Works well for large datasets and deep networks.  \n",
    "\n",
    "**Why:**  \n",
    "- Solves the vanishing gradient problem (partially).  \n",
    "- Computationally efficient.  \n",
    "\n",
    "**Example:**  \n",
    "Image recognition (hidden layers in CNN).  \n",
    "\n",
    "\n",
    "## **Leaky ReLU**\n",
    "\n",
    "- Formula:  \n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "where α is a small value (e.g., 0.01).  \n",
    "\n",
    "- Range: (-∞, ∞).  \n",
    "- Allows small negative values.  \n",
    "\n",
    "**Where to use:**  \n",
    "- Hidden layers when we want to avoid “dead neurons” (ReLU problem where some neurons output 0 always).  \n",
    "\n",
    "**Example:**  \n",
    "Used in deep CNNs where many layers might cause neurons to die.  \n",
    "\n",
    "## **Softmax Function**\n",
    "\n",
    "- Formula:  \n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$  \n",
    "\n",
    "- Converts raw outputs (logits) into **probabilities**.  \n",
    "- Range: (0, 1), and all outputs add up to 1.  \n",
    "\n",
    "**Where to use:**  \n",
    "- Always used in the **output layer for multiclass classification**.  \n",
    "- Example: Predicting whether an image is a **cat, dog, or horse**.  \n",
    "\n",
    "\n",
    "## Small Intuitive Examples\n",
    "\n",
    "> **Example 1: Sigmoid (Binary Classification)**\n",
    "\n",
    "- Input: Student studies 5 hours.  \n",
    "- Output: Probability of passing = 0.85 → Student likely passes.  \n",
    "\n",
    "> **Example 2: Tanh**\n",
    "\n",
    "- Input: Movie review text.  \n",
    "- Output: -0.9 → strongly negative review.  \n",
    "\n",
    "> **Example 3: ReLU**\n",
    "\n",
    "- Input: Pixel value in an image = -20 → Output = 0.  \n",
    "- Input: Pixel value = 150 → Output = 150.  \n",
    "\n",
    "> **Example 4: Leaky ReLU**\n",
    "\n",
    "- Input: -20 → Output = -0.2 (small negative allowed).  \n",
    "\n",
    "> **Example 5: Softmax (Multiclass)**\n",
    "\n",
    "- Input Image: Could be Cat, Dog, or Horse.  \n",
    "- Output: [Cat=0.7, Dog=0.2, Horse=0.1].  \n",
    "- Prediction = Cat.  \n",
    "\n",
    "## Summary of Activation Functions\n",
    "\n",
    "| Activation | Range       | Where Used | Example Use Case |\n",
    "|------------|------------|------------|------------------|\n",
    "| Sigmoid    | (0, 1)     | Output layer (binary classification) | Spam email detection |\n",
    "| Tanh       | (-1, 1)    | Hidden layers / output | Sentiment analysis |\n",
    "| ReLU       | [0, ∞)     | Hidden layers | Image recognition |\n",
    "| Leaky ReLU | (-∞, ∞)    | Hidden layers (avoid dead neurons) | Deep CNNs |\n",
    "| Softmax    | (0, 1) sum=1 | Output layer (multiclass classification) | Cat vs Dog vs Horse |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8790c73-7881-4f24-a5c5-b1016894e450",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Deep Learning is a collection of **neural network architectures (ANN, CNN, RNN)**.  \n",
    "- Neural networks are inspired by the **human brain**.  \n",
    "- Key concepts:  \n",
    "  - Neuron  \n",
    "  - Neural Network  \n",
    "  - Perceptron (Synapse)  \n",
    "  - Input Layer, Hidden Layer, Output Layer  \n",
    "  - Forward Propagation & Backward Propagation  \n",
    "  - Epochs  \n",
    "  - Activation Functions (Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4023a66-f6d6-4563-9c1a-63cfec01a4b2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we introduced the foundation of Deep Learning:\n",
    "\n",
    "- What Deep Learning is and why it is used.  \n",
    "- Neural Network Architecture (Input, Hidden, Output layers).  \n",
    "- Key concepts: Neuron, Perceptron, Forward & Backward Propagation, Epochs.  \n",
    "- Activation Functions (Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax).  \n",
    "\n",
    "Deep Learning is inspired by the human brain and forms the backbone of modern AI systems such as speech recognition, computer vision, and natural language processing.  \n",
    "\n",
    "This is just the beginning   \n",
    "\n",
    "In the next notes, I’ll document more about **Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN)** with real-world examples.  \n",
    "\n",
    "Thanks for reading!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
