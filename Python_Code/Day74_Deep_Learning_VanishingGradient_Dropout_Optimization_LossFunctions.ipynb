{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ab726b-1d38-467b-b791-1bc481c3e300",
   "metadata": {},
   "source": [
    "# Deep Learning (Vanishing Gradient, Dropout, Optimization, Loss Functions)\n",
    "\n",
    "Welcome back to my Deep Learning documentation!  \n",
    "\n",
    "In this notebook, we continue from **Day73**, focusing on important training concepts in neural networks:  \n",
    "\n",
    "- Vanishing Gradient Problem  \n",
    "- Chain Rule in Backpropagation  \n",
    "- Dropout Neurons  \n",
    "- Optimization Techniques  \n",
    "- Loss Functions  \n",
    "\n",
    "\n",
    "#  Vanishing Gradient Problem\n",
    "\n",
    "Deep neural networks often face the **Vanishing Gradient Problem** during training.  \n",
    "\n",
    "## What is it?  \n",
    "- In deep networks, gradients become **very small** as they propagate backward.  \n",
    "- This makes weight updates negligible in early layers → **network stops learning**.  \n",
    "\n",
    "## Why does it happen?  \n",
    "- Sigmoid and Tanh activations squash values into a small range.  \n",
    "- Their derivatives are small:  \n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x)) \\quad \\in (0, 0.25)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^2(x) \\quad \\in (0, 1)\n",
    "$$  \n",
    "\n",
    "- Multiplying many small derivatives → gradient approaches 0.  \n",
    "\n",
    "## Symptoms  \n",
    "- Accuracy stops improving after a few epochs.  \n",
    "- Model seems \"stuck\" at some accuracy.  \n",
    "\n",
    "## Solutions  \n",
    "1. Use **ReLU / Leaky ReLU** instead of Sigmoid/Tanh.  \n",
    "2. Apply **Batch Normalization** → keeps activations stable.  \n",
    "3. Use better **optimizers** (Adam, RMSProp).  \n",
    "4. **Dropout Neurons** → improves generalization.  \n",
    "\n",
    "#  Chain Rule in Backpropagation\n",
    "\n",
    "Backpropagation is based on the **Chain Rule of Calculus**.  \n",
    "\n",
    "## Formula  \n",
    "If:  \n",
    "$$\n",
    "y = f(g(x))\n",
    "$$  \n",
    "\n",
    "Then derivative:  \n",
    "$$\n",
    "\\frac{dy}{dx} = f'(g(x)) \\cdot g'(x)\n",
    "$$  \n",
    "\n",
    "## In Neural Networks  \n",
    "- Error flows backward from **Output → Hidden → Input**.  \n",
    "- Each layer’s gradient is computed as a product of partial derivatives.  \n",
    "\n",
    "Example (simple 2-layer NN):  \n",
    "\n",
    "$$\n",
    "L = f(z), \\quad z = g(h), \\quad h = w \\cdot x\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw} = \\frac{dL}{dz} \\cdot \\frac{dz}{dh} \\cdot \\frac{dh}{dw}\n",
    "$$  \n",
    "\n",
    "This shows how gradients are **chained** across layers.  \n",
    "\n",
    "\n",
    "#  Dropout Neurons\n",
    "\n",
    "Dropout is a **regularization technique** used to prevent overfitting.  \n",
    "\n",
    "## What is Dropout?  \n",
    "- During training, **random neurons are dropped** (set to 0).  \n",
    "- This forces the network to not depend on specific neurons.  \n",
    "\n",
    "## Example  \n",
    "- Suppose a layer has 100 neurons.  \n",
    "- If dropout = 0.5 → only ~50 neurons are active at each step.  \n",
    "\n",
    "## At Inference (Testing)  \n",
    "- No dropout is applied.  \n",
    "- Weights are **scaled** to match training conditions.  \n",
    "\n",
    " Dropout improves generalization and reduces overfitting.  \n",
    "\n",
    "\n",
    "#  Optimization in Deep Learning\n",
    "\n",
    "Optimizers control **how weights are updated** during training.  \n",
    "\n",
    "##  Types of Gradient Descent  \n",
    "\n",
    "### Stochastic Gradient Descent (SGD)  \n",
    "- Updates weights after **each sample**.  \n",
    "- Faster but noisy.  \n",
    "\n",
    "### Batch Gradient Descent (BGD)  \n",
    "- Uses the **entire dataset** for each update.  \n",
    "- Very accurate but slow.  \n",
    "\n",
    "### Mini-Batch Gradient Descent  \n",
    "- Uses small batches of data.  \n",
    "- Default choice in practice.  \n",
    "\n",
    "\n",
    "##  Advanced Optimizers  \n",
    "\n",
    "### Adam (Adaptive Moment Estimation)  \n",
    "- Combines **momentum + adaptive learning rate**.  \n",
    "- Very popular for deep learning.  \n",
    "\n",
    "### Adamax  \n",
    "- Variant of Adam.  \n",
    "- Works better with **sparse gradients**.  \n",
    "\n",
    "### Adadelta  \n",
    "- Dynamically adjusts learning rate.  \n",
    "\n",
    "### RMSProp  \n",
    "- Keeps moving average of squared gradients.  \n",
    "- Prevents exploding/vanishing updates.  \n",
    "\n",
    "\n",
    "#  Loss Functions\n",
    "\n",
    "Loss functions measure **how far predictions are from actual values**.  \n",
    "\n",
    "##  Regression Losses  \n",
    "\n",
    "### Mean Absolute Error (MAE)  \n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_{true}^{(i)} - y_{pred}^{(i)}|\n",
    "$$  \n",
    "\n",
    "### Mean Squared Error (MSE)  \n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{true}^{(i)} - y_{pred}^{(i)}\\big)^2\n",
    "$$  \n",
    "\n",
    "### Log Loss  \n",
    "- Used for probabilistic regression.  \n",
    "\n",
    "\n",
    "##  Classification Losses  \n",
    "\n",
    "### Binary Cross-Entropy  \n",
    "For binary classification (0/1):  \n",
    "\n",
    "$$\n",
    "Loss = -\\frac{1}{n} \\sum_{i=1}^{n} \\Big[ y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\Big]\n",
    "$$  \n",
    "\n",
    "### Categorical Cross-Entropy  \n",
    "For multi-class classification (one-hot labels):  \n",
    "\n",
    "$$\n",
    "Loss = -\\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$  \n",
    "\n",
    "### Sparse Categorical Cross-Entropy  \n",
    "- Similar to categorical cross-entropy, but labels are **integers** instead of one-hot.  \n",
    "\n",
    "\n",
    "#  Key Insights\n",
    "\n",
    "- Backpropagation = **Forward Propagation + Chain Rule + Weight Updates**.  \n",
    "- If weights stop updating → **Vanishing Gradient Problem**.  \n",
    "- **Dropout Neurons** + **L1/L2 Regularization** help reduce overfitting.  \n",
    "- Advanced optimizers (Adam, RMSProp, etc.) improve convergence speed.  \n",
    "- Different loss functions are used for **regression vs classification** tasks.  \n",
    "\n",
    "\n",
    "#  Summary\n",
    "\n",
    "- Vanishing Gradient slows training → solved with ReLU, Dropout, BatchNorm, better optimizers.  \n",
    "- Chain Rule = backbone of backpropagation.  \n",
    "- Dropout Neurons = prevent overfitting.  \n",
    "- Optimizers = decide how learning happens.  \n",
    "- Loss Functions = measure error in regression/classification.  \n",
    "\n",
    "\n",
    "\n",
    "#  Conclusion\n",
    "\n",
    "In this notebook, we covered:  \n",
    "- Vanishing Gradient Problem  \n",
    "- Chain Rule in Backpropagation  \n",
    "- Dropout Neurons  \n",
    "- Optimization Methods (SGD, Adam, RMSProp, etc.)  \n",
    "- Loss Functions (Regression & Classification)  \n",
    "\n",
    " These concepts help us train **deeper and more accurate neural networks**.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
