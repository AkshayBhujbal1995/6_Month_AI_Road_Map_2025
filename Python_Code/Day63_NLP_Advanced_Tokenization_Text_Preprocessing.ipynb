{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5353885-a505-40b4-b87f-94ab79ec8294",
   "metadata": {},
   "source": [
    "**Advanced Tokenization, N-grams, Stemming, Lemmatization & Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a637ae-8d86-4366-bbed-6fd76d2118e6",
   "metadata": {},
   "source": [
    "Today, we continued our journey into Natural Language Processing (NLP) by exploring more tokenization techniques, generating n-grams, and learning about stemming, lemmatization, and stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bac10-ba68-416e-bd69-87aef1d0e7fe",
   "metadata": {},
   "source": [
    "# Whitespace Tokenization\n",
    "\n",
    "**Definition:** Splits text based on whitespace (spaces, tabs, newlines) without removing punctuation.\n",
    "\n",
    "- Useful when punctuation should be preserved as part of the tokens.\n",
    "\n",
    "- Faster but less precise than other tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f620cc0-698a-4db0-8b61-787a83b81f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also create your own words\n",
    "AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of\n",
    "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and\n",
    "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.\n",
    "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
    "AI could solve major challenges and crisis situations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1244e319-36bd-412c-b10c-db23a7f1a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals.', 'With', 'Artificial', 'Intelligence,', 'machines', 'perform', 'functions', 'such', 'as', 'learning,', 'planning,', 'reasoning', 'and', 'problem-solving.', 'Most', 'noteworthy,', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines.', 'It', 'is', 'probably', 'the', 'fastest-growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation.', 'Furthermore,', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wt = WhitespaceTokenizer().tokenize(AI)\n",
    "print(wt)  # Clean split by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1a055d-75d2-4a5c-96f4-73601107b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(wt))  # Count of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef1c07-1031-42d0-a5ca-6c6eb2d962c2",
   "metadata": {},
   "source": [
    "# WordPunct Tokenization\n",
    "**Definition:** Splits words and punctuation into separate tokens.\n",
    "\n",
    "- Numbers, punctuation marks, and words are treated individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c410985-a82d-4b01-9120-be359425c858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "s = 'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26265c39-226d-4d30-be46-6ea433b06dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'apple', 'cost', '$', '3', '.', '88', 'in', 'Hyderabad', '.', 'Please', 'buy', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42e3cc7-e01b-489f-9827-aabc3723f257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(wordpunct_tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefc0774-b1c1-495a-86af-aaf0cc884c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines', '.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals', '.', 'With', 'Artificial', 'Intelligence', ',', 'machines', 'perform', 'functions', 'such', 'as', 'learning', ',', 'planning', ',', 'reasoning', 'and', 'problem', '-', 'solving', '.', 'Most', 'noteworthy', ',', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines', '.', 'It', 'is', 'probably', 'the', 'fastest', '-', 'growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation', '.', 'Furthermore', ',', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations', '.']\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "w_p = wordpunct_tokenize(AI)\n",
    "print(w_p)\n",
    "print(len(w_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7902-e8a3-4946-ad03-d88c5bb4c443",
   "metadata": {},
   "source": [
    "**Summary of Tokenizers**\n",
    "\n",
    "| Tokenizer             | Description                                   |\n",
    "| --------------------- | --------------------------------------------- |\n",
    "| `word_tokenize`       | Splits into words and punctuation (accurate). |\n",
    "| `sent_tokenize`       | Splits into sentences.                        |\n",
    "| `blankline_tokenize`  | Splits into paragraphs by blank lines.        |\n",
    "| `WhitespaceTokenizer` | Splits by spaces/tabs only.                   |\n",
    "| `wordpunct_tokenize`  | Splits words and punctuation separately.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a765be-5237-4195-9041-eedea650145a",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "**Definition:** Sequences of n consecutive tokens.\n",
    "\n",
    "- Bigram: 2-word sequence.\n",
    "- Trigram: 3-word sequence.\n",
    "- N-gram: Any n-word sequence (n > 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84f76ef-b207-4bfc-a927-b70d9f64a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are learner of AI from 4th May 2025 till now'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "string = \"we are learner of AI from 4th May 2025 till now\"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0c7944c-482e-428a-b7b4-3926d7f2c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'learner', 'of', 'AI', 'from', '4th', 'May', '2025', 'till', 'now']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "\n",
    "print(quotes_tokens)\n",
    "print(len(quotes_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d38133-2827-4ac0-8b7d-3f7f7ef671cb",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38028bf9-571d-4e9f-8298-1e7bba55c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'are'), ('are', 'learner'), ('learner', 'of'), ('of', 'AI'), ('AI', 'from'), ('from', '4th'), ('4th', 'May'), ('May', '2025'), ('2025', 'till'), ('till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# Bigrams\n",
    "quotes_tokens_bi = list(nltk.bigrams(quotes_tokens))\n",
    "print(quotes_tokens_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2dc8d-54f0-4ce9-9346-13fdd490caa8",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52f5ff5-88b5-4298-975a-6ab54eeca18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'are', 'learner'), ('are', 'learner', 'of'), ('learner', 'of', 'AI'), ('of', 'AI', 'from'), ('AI', 'from', '4th'), ('from', '4th', 'May'), ('4th', 'May', '2025'), ('May', '2025', 'till'), ('2025', 'till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# Trigrams\n",
    "quotes_tokens_tri = list(nltk.trigrams(quotes_tokens))\n",
    "print(quotes_tokens_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd8815-5749-4e9c-95c0-e65fabb175d0",
   "metadata": {},
   "source": [
    "## N-grams (n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8837952-bdf5-4ec9-94ac-3b2fb51eb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('we', 'are', 'learner', 'of', 'AI', 'from', '4th', 'May'), ('are', 'learner', 'of', 'AI', 'from', '4th', 'May', '2025'), ('learner', 'of', 'AI', 'from', '4th', 'May', '2025', 'till'), ('of', 'AI', 'from', '4th', 'May', '2025', 'till', 'now')]\n"
     ]
    }
   ],
   "source": [
    "# N-grams (n=8)\n",
    "quotes_tokens_n = list(nltk.ngrams(quotes_tokens, 8))\n",
    "print(quotes_tokens_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0258bf0-bb90-4834-b9b1-782aadbb9eb8",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "**Definition:** Reduces words to their root form, often by chopping off suffixes.\n",
    "Types:\n",
    "\n",
    "- **Porter Stemmer** – Basic and widely used, but may not handle all words well.\n",
    "- **Lancaster Stemmer** – More aggressive, sometimes over-stems words.\n",
    "- **Snowball Stemmer** – Advanced, supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cebe0edf-7449-4219-8697-30b91fcc02e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['given',\n",
       " 'give',\n",
       " 'giving',\n",
       " 'gave',\n",
       " 'thinking',\n",
       " 'loving',\n",
       " 'maximum',\n",
       " 'akshaybhujbal',\n",
       " 'gaved']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "words_to_stem = ['given','give','giving','gave','thinking','loving','maximum','akshaybhujbal','gaved']\n",
    "words_to_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0a067-fed0-4b27-811b-2119e5081bbe",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3ecf1bf-4086-4a2b-9f20-9d53d80e5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : give\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "maximum : maximum\n",
      "akshaybhujbal : akshaybhujb\n",
      "gaved : gave\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "pst = PorterStemmer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + pst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6445b-8aa5-45db-838e-e9d8df5bb460",
   "metadata": {},
   "source": [
    "## Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2cf82e3-1af3-4e87-8961-b48ed752c6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : giv\n",
      "give : giv\n",
      "giving : giv\n",
      "gave : gav\n",
      "thinking : think\n",
      "loving : lov\n",
      "maximum : maxim\n",
      "akshaybhujbal : akshaybhujb\n",
      "gaved : gav\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "lst = LancasterStemmer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091e3ed-6e5d-4658-b6d9-9e91639f6740",
   "metadata": {},
   "source": [
    "## Snowball Stemmer (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f63821e-f7f5-4ea6-ac08-9bc5c6c85ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : give\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "maximum : maximum\n",
      "akshaybhujbal : akshaybhujb\n",
      "gaved : gave\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer (English)\n",
    "sbst = SnowballStemmer('english')\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + sbst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b461eb5-2040-45da-ac48-4ffd3c84b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samstag\n"
     ]
    }
   ],
   "source": [
    "# Snowball Stemmer (German)\n",
    "stemmer = SnowballStemmer('german')\n",
    "print(stemmer.stem('samstag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a670c-5f9a-4701-ab1f-6864dbbb2373",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "**Definition:** Reduces words to their dictionary (lemma) form, considering meaning and grammar.\n",
    "\n",
    "- More accurate than stemming because it uses vocabulary and morphological analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee00dc9d-c209-4db4-b845-b2f328fb3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given : given\n",
      "give : give\n",
      "giving : giving\n",
      "gave : gave\n",
      "thinking : thinking\n",
      "loving : loving\n",
      "maximum : maximum\n",
      "akshaybhujbal : akshaybhujbal\n",
      "gaved : gaved\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lem = WordNetLemmatizer()\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1288be-d6b6-471b-857c-0e7bb5daf846",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "**Definition:** Commonly used words (e.g., \"the\", \"is\", \"in\") that are often removed in NLP tasks.\n",
    "- Removing stopwords helps focus on meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0acd97f-5703-40b2-bb7a-0f723d29bb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ed2b33a-a035-40bd-82d4-4ed4c24f1736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2354c4a1-0ae8-44e6-8432-304b131dee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords length for French is 157\n",
      "Stopwords length for German is 232\n",
      "Stopwords length for Chinese is 841\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"Stopwords length for French is\", len(stopwords.words('french')))\n",
    "print(\"Stopwords length for German is\", len(stopwords.words('german')))\n",
    "print(\"Stopwords length for Chinese is\", len(stopwords.words('chinese')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2ba74-f290-49f5-8a14-41f119914fc3",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "  \n",
    "* Learned WhitespaceTokenizer and wordpunct_tokenize for flexible token splitting.\n",
    "* Explored n-grams for sequence-based analysis.\n",
    "* Compared Porter, Lancaster, and Snowball stemmers.\n",
    "* Understood lemmatization for accurate root forms.\n",
    "* Reviewed stopwords in multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f12b2-b286-4a9a-9690-fd7f4bf1ce48",
   "metadata": {},
   "source": [
    "# **NLP Quick Reference – Tokenization, N-grams, Stemming, Lemmatization, Stopwords**\n",
    "\n",
    "## Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4656e383-0493-4316-8c0d-96ac23b03668",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Natural Language Processing makes it possible for machines \n",
    "to understand human language and perform useful tasks.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620331df-a3e6-4f72-aa32-aee7c5b33e6b",
   "metadata": {},
   "source": [
    "## Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db76f537-d294-4b42-a9db-d0e1573b6eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'for',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'useful',\n",
       " 'tasks',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, blankline_tokenize\n",
    "\n",
    "word_tokenize(text)         # Words + punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbf91a3a-b219-4784-885b-9bafbc2edcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing makes it possible for machines \\nto understand human language and perform useful tasks.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)         # Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0619e2b-7bdd-4666-979c-eb79eccd95b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing makes it possible for machines \\nto understand human language and perform useful tasks.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blankline_tokenize(text)    # Paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faed3c-4be0-4c1b-a0ac-0f30bf9b9275",
   "metadata": {},
   "source": [
    "## Whitespace Tokenizer\n",
    "Splits only on spaces/tabs, keeps punctuation attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fd49e60-1efa-488f-98f6-99f7a0dbfa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'for',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'useful',\n",
       " 'tasks.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WhitespaceTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f911ab-8a76-482a-b62e-43927897fb72",
   "metadata": {},
   "source": [
    "## WordPunct Tokenizer\n",
    "Splits words and punctuation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f94aaa16-669c-4cbf-ab2c-11f9f0fa19de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'for',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'human',\n",
       " 'language',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'useful',\n",
       " 'tasks',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f95c64-fb3d-4ac0-a36e-d6298d3929fa",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "Create sequences of n consecutive tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e563d5ed-2499-4e09-8372-4734affb09b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'Language'),\n",
       " ('Language', 'Processing'),\n",
       " ('Processing', 'makes'),\n",
       " ('makes', 'it'),\n",
       " ('it', 'possible'),\n",
       " ('possible', 'for'),\n",
       " ('for', 'machines'),\n",
       " ('machines', 'to'),\n",
       " ('to', 'understand'),\n",
       " ('understand', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'and'),\n",
       " ('and', 'perform'),\n",
       " ('perform', 'useful'),\n",
       " ('useful', 'tasks'),\n",
       " ('tasks', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "list(nltk.bigrams(tokens))        # Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc725cc0-7dc9-496b-a15c-36755d8bdfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'Language', 'Processing'),\n",
       " ('Language', 'Processing', 'makes'),\n",
       " ('Processing', 'makes', 'it'),\n",
       " ('makes', 'it', 'possible'),\n",
       " ('it', 'possible', 'for'),\n",
       " ('possible', 'for', 'machines'),\n",
       " ('for', 'machines', 'to'),\n",
       " ('machines', 'to', 'understand'),\n",
       " ('to', 'understand', 'human'),\n",
       " ('understand', 'human', 'language'),\n",
       " ('human', 'language', 'and'),\n",
       " ('language', 'and', 'perform'),\n",
       " ('and', 'perform', 'useful'),\n",
       " ('perform', 'useful', 'tasks'),\n",
       " ('useful', 'tasks', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(tokens))       # Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a011e2a2-85f3-4dc4-be67-a1c4ac92f45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'Language', 'Processing', 'makes'),\n",
       " ('Language', 'Processing', 'makes', 'it'),\n",
       " ('Processing', 'makes', 'it', 'possible'),\n",
       " ('makes', 'it', 'possible', 'for'),\n",
       " ('it', 'possible', 'for', 'machines'),\n",
       " ('possible', 'for', 'machines', 'to'),\n",
       " ('for', 'machines', 'to', 'understand'),\n",
       " ('machines', 'to', 'understand', 'human'),\n",
       " ('to', 'understand', 'human', 'language'),\n",
       " ('understand', 'human', 'language', 'and'),\n",
       " ('human', 'language', 'and', 'perform'),\n",
       " ('language', 'and', 'perform', 'useful'),\n",
       " ('and', 'perform', 'useful', 'tasks'),\n",
       " ('perform', 'useful', 'tasks', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens, 4))      # 4-word n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ec276-daa2-4c2a-86ae-1f5879b5f8b9",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Reduce words to root form (may not be real words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d23c914b-f912-46be-94ca-9e6f43a79cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "words = ['running', 'runs', 'easily', 'fairly']\n",
    "\n",
    "PorterStemmer().stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06ada219-bf79-4dbe-9037-4d26f08c7231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LancasterStemmer().stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac049bfe-6685-4b27-9366-5593dc938b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer('english').stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9136f7-3f34-4ab7-81c1-a0809cd3ecaf",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Reduce to dictionary form, considering meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4c6b6cf-4aa9-4306-aa9a-e784b420d467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "WordNetLemmatizer().lemmatize('running', pos='v')  # 'run'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe01b38-9603-4ff4-a3ff-74a8e4e5fc4f",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Common words often removed in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2986d8e-92d0-48e9-bbfe-b4bb866e7742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]   # First 10 stopwords\n",
    "len(stopwords.words('english'))   # Count of English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84841b66-0b3e-4233-a78b-32026d70fa2c",
   "metadata": {},
   "source": [
    "**Closing Notes :**\n",
    "\n",
    "- Tokenization is the **first step** in almost every NLP task.  \n",
    "- The choice between **stemming** and **lemmatization** depends on whether you need speed (stemming) or accuracy (lemmatization).  \n",
    "- **Stopwords removal** helps focus on meaningful words, but always check if they are important for your specific task.  \n",
    "- **N-grams** capture word sequences and context, useful for tasks like text prediction or sentiment analysis.  \n",
    "- Always experiment with different tokenizers and preprocessing techniques to see what works best for your dataset.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
