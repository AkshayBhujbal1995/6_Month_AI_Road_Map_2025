{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43a9fa5-a347-44dc-a5c9-2eaa414113c2",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks RNN\n",
    "\n",
    "Recurrent Neural Networks RNN are a type of neural network designed for sequential data such as text speech time series and video. Unlike feedforward neural networks RNNs have connections that form directed cycles which allow them to maintain memory of previous inputs. This makes RNNs very effective for tasks where context and order of inputs are important.\n",
    "\n",
    "RNNs are widely used for natural language processing speech recognition machine translation and other sequence related tasks.\n",
    "\n",
    "# RNN Architecture\n",
    "\n",
    "The key idea behind an RNN is that it processes input sequences step by step while maintaining a hidden state that carries information about previous inputs. This hidden state is updated at every step and passed to the next step along with the new input.\n",
    "\n",
    "At each time step the RNN takes two inputs  \n",
    "    \n",
    "- The current input vector xt  \n",
    "- The hidden state from the previous time step ht-1  \n",
    "\n",
    "It then produces \n",
    "\n",
    "- The updated hidden state ht  \n",
    "- An output yt  \n",
    "\n",
    "The recurrence relation can be written as  \n",
    "ht = f(Wxh xt + Whh ht-1 + bh)  \n",
    "yt = Why ht + by  \n",
    "\n",
    "Here f is usually a nonlinear activation function such as tanh or ReLU.\n",
    "\n",
    "\n",
    "# Sequence to Sequence Neural Network\n",
    "\n",
    "When RNNs are used for tasks like machine translation or chatbot building we use an encoder decoder architecture. This is called a sequence to sequence model.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The encoder reads the input sequence one token at a time and compresses the entire sequence into a fixed length vector called the context vector. This vector represents the meaning or features of the input sequence.\n",
    "\n",
    "## Decoder\n",
    "\n",
    "The decoder takes the context vector from the encoder and generates the output sequence step by step. At each step it predicts the next token in the sequence until the final output is produced. Decoders are also implemented using RNNs LSTMs or GRUs.\n",
    "\n",
    "# LSTM Long Short Term Memory\n",
    "\n",
    "A problem with simple RNNs is that they suffer from vanishing or exploding gradients when trained on long sequences. This makes it hard for them to remember information from far in the past.\n",
    "\n",
    "To solve this the Long Short Term Memory network LSTM was introduced. LSTMs have a special architecture that allows them to learn long range dependencies. They use gates to control the flow of information.\n",
    "\n",
    "## Gates in LSTM\n",
    "\n",
    "**Input gate** \n",
    "\n",
    "Controls how much new information from the current input flows into the memory cell  \n",
    "\n",
    "**Forget gate**  \n",
    "\n",
    "Controls how much of the past memory should be forgotten or retained  \n",
    "\n",
    "**Output gate**\n",
    "\n",
    "Controls how much information from the memory cell should be passed to the hidden state  \n",
    "\n",
    "**Memory cell**  \n",
    "\n",
    "Acts as a conveyor belt that carries information across time steps with only minor modifications  \n",
    "\n",
    "These gates allow the LSTM to selectively remember or forget information which solves the vanishing gradient problem.\n",
    "\n",
    "# GRU Gated Recurrent Unit\n",
    "\n",
    "The Gated Recurrent Unit GRU is a simplified version of the LSTM. It combines the input gate and forget gate into a single update gate and also has a reset gate.\n",
    "\n",
    "## Gates in GRU\n",
    "\n",
    "**Update gate**  \n",
    "Decides how much of the past information needs to be passed along to the future  \n",
    "\n",
    "**Reset gate**  \n",
    "Decides how much of the past information to forget  \n",
    "\n",
    "GRUs are simpler and computationally faster than LSTMs while performing similarly well on many tasks.\n",
    "\n",
    "# Applications of RNN LSTM and GRU\n",
    "\n",
    "Recurrent Neural Networks and their variants LSTM and GRU are widely used in tasks that involve sequential data where the order of information matters. Below are some important applications.\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "RNNs are heavily used in text related tasks such as language modeling sentiment analysis machine translation text summarization and question answering. LSTM and GRU help in capturing long term dependencies in language which simple RNNs cannot handle effectively.\n",
    "\n",
    "## Next Word Prediction\n",
    "\n",
    "Given a sequence of words the model predicts the next likely word. This is useful in predictive keyboards autocomplete systems and writing assistants. LSTMs are particularly effective in this application because they can remember context over many words.\n",
    "\n",
    "## Machine Translation\n",
    "\n",
    "Using encoder decoder RNN architectures we can translate sentences from one language to another. The encoder processes the input sentence and the decoder generates the translated sentence in the target language.\n",
    "\n",
    "## Speech Recognition\n",
    "\n",
    "RNNs can process audio signals as sequential data. By mapping audio frames to corresponding phonemes or words they can convert spoken language into text. LSTMs and GRUs are widely used in modern speech recognition systems.\n",
    "\n",
    "## Chatbots and Conversational AI\n",
    "\n",
    "Chatbots use sequence to sequence RNN models where the encoder processes the user query and the decoder generates a human like response. They can be trained on large conversation datasets to learn natural conversation flow.\n",
    "\n",
    "## Text Summarization\n",
    "\n",
    "RNNs can generate concise summaries of long documents. The encoder reads the entire text and the decoder outputs a shorter version that retains the main ideas.\n",
    "\n",
    "## Time Series Forecasting\n",
    "\n",
    "RNNs can analyze patterns in time series data such as stock prices weather data and sales data to predict future values. LSTMs are preferred for this task as they capture long range temporal dependencies.\n",
    "\n",
    "## Music Generation\n",
    "\n",
    "RNNs can be trained on sequences of musical notes to generate new pieces of music. Given a starting sequence the model can predict the next notes and create original compositions.\n",
    "\n",
    "## Handwriting Recognition and Generation\n",
    "\n",
    "RNNs can be trained to recognize handwritten text by processing sequences of strokes or pixels. They can also generate handwriting styles when trained on a dataset of handwritten samples.\n",
    "\n",
    "# Summary of Applications\n",
    "\n",
    "RNNs LSTMs and GRUs are extremely versatile and are used in tasks involving language speech vision and time series.  \n",
    "They are the backbone of many real world AI applications like Google Translate Siri predictive keyboards and conversational agents.\n",
    "\n",
    "\n",
    "# Summary\n",
    "\n",
    "RNNs are powerful models for handling sequential data.  \n",
    "Basic RNNs capture short term dependencies but struggle with long sequences due to vanishing gradients.  \n",
    "LSTMs solve this with memory cells and gating mechanisms while GRUs provide a simpler alternative.  \n",
    "They are widely applied in next word prediction speech recognition and chatbot systems using encoder decoder architectures.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
