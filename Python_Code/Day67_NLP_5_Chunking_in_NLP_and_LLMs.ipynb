{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acf8884-4950-41d1-9673-433b78268088",
   "metadata": {},
   "source": [
    "**Chunking in NLP & LLMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0a1f6-35a8-4ee1-9c75-8fb49651bc17",
   "metadata": {},
   "source": [
    "**What is Chunking?**\n",
    "\n",
    "- **Chunking** is the process of dividing text into **meaningful segments** called **chunks**.  \n",
    "- Unlike POS tagging (which assigns a tag to each word), **chunking groups words together** to form higher-level units like Noun Phrases, Verb Phrases, and Prepositional Phrases.  \n",
    "- Example:  \n",
    "  - \"The black dog\" → **Noun Phrase (NP)**  \n",
    "  - \"is running\" → **Verb Phrase (VP)**  \n",
    "  - \"in the park\" → **Prepositional Phrase (PP)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ef843-77e8-49c0-8b34-ab5560773906",
   "metadata": {},
   "source": [
    "**Why is Chunking Important?**\n",
    "    \n",
    "- Helps extract meaningful **phrases** instead of individual words.  \n",
    "- Useful in **Information Extraction, Named Entity Recognition, Question Answering, LLM preprocessing**.  \n",
    "- Interview Tip  → *“Explain Chunking in NLP and in LLMs”* is a common question.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11853-a136-41f0-b50e-e1b90a77cb15",
   "metadata": {},
   "source": [
    "# Chunking in NLP using NLTK\n",
    "We’ll use **NLTK** to create and visualize chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94955781-0e02-4574-b571-0021a6eb651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "# Download required models only onces\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d790e75-32fc-43dc-8a3e-0226b9ad5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"full stack datascience, generative ai, agenti ai, llm model keep increase by different compnay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e449a190-23f2-41d4-9260-c3d5a50e48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f495b72-0f73-4a08-a41a-e518bcc8f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "tagged_tokens = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7cff8a-23e5-4da5-b509-1eaa4ce0ec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Tokens:\n",
      " [('full', 'JJ'), ('stack', 'NN'), ('datascience', 'NN'), (',', ','), ('generative', 'JJ'), ('ai', 'NN'), (',', ','), ('agenti', 'NN'), ('ai', 'NN'), (',', ','), ('llm', 'JJ'), ('model', 'NN'), ('keep', 'VB'), ('increase', 'NN'), ('by', 'IN'), ('different', 'JJ'), ('compnay', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged Tokens:\\n\", tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c6bc70-778a-4c01-b433-32100116b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk grammar\n",
    "chunk_grammar = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>}          # Noun Phrase\n",
    "VP: {<VB.*><NP|PP>*}          # Verb Phrase\n",
    "PP: {<IN><NP>}                # Prepositional Phrase\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057ececb-f49c-4073-bbfa-bb76ad1be492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chunk parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c40a2fd4-e707-46e9-893e-ba6ac018a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the tagged tokens\n",
    "chunked = chunk_parser.parse(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d503ef17-a3b8-4025-8247-69936792dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunked Output:\n",
      " (S\n",
      "  (NP full/JJ stack/NN)\n",
      "  (NP datascience/NN)\n",
      "  ,/,\n",
      "  (NP generative/JJ ai/NN)\n",
      "  ,/,\n",
      "  (NP agenti/NN)\n",
      "  (NP ai/NN)\n",
      "  ,/,\n",
      "  (NP llm/JJ model/NN)\n",
      "  (VP keep/VB (NP increase/NN))\n",
      "  (PP by/IN (NP different/JJ compnay/NN)))\n"
     ]
    }
   ],
   "source": [
    "# Print and visualize\n",
    "print(\"\\nChunked Output:\\n\", chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f92bc-fdd4-426d-90e3-f20f56968ca3",
   "metadata": {},
   "source": [
    "# Chunking Visualization (NLTK Output)\n",
    "![Chunking Output](3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df157e-b617-4d97-ac61-7d875928c800",
   "metadata": {},
   "source": [
    "# Chunking in LLMs\n",
    "\n",
    "- In **NLP with NLTK**, chunking = grouping tokens into phrases.  \n",
    "- In **LLMs (Large Language Models)**, chunking = splitting **long text into smaller parts (chunks)** so that models like GPT can process them within their **context window**.  \n",
    "\n",
    "**Why?**\n",
    "    \n",
    "- LLMs (like GPT-2, GPT-3, BERT, T5) have a **fixed token limit**.  \n",
    "- Chunking long documents ensures the model can process text without losing information.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c16cfa-aaaa-45d5-8fa1-610a32afed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note on Running LLM Chunking on CPU\n",
    "\n",
    "- GPT-2 and similar models are large, so running them on CPU may cause **out-of-memory** or **very slow performance**.  \n",
    "- To avoid errors:  \n",
    "  - Use smaller models like **distilgpt2**.  \n",
    "  - Explicitly load the model on CPU.  \n",
    "  - Keep `max_length` small (like 50–100).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c25e1-467a-41c9-948f-f48ca54205ab",
   "metadata": {},
   "source": [
    "## Running Larger LLMs on **Google Colab (GPU)** — Documentation Only\n",
    "\n",
    "> **Note:** This section is for **documentation** inside your notebook.  \n",
    "> We **haven’t executed** these cells here or shown outputs.  \n",
    "> To run them, open **Google Colab**, set **Runtime → GPU**, then **copy-paste each cell** below into separate Colab cells **in order**.\n",
    "\n",
    "\n",
    "**Step 0 — Switch Colab to GPU**\n",
    "    \n",
    "**Colab menu:** `Runtime` → `Change runtime type` → **Hardware accelerator:** `GPU` → **Save**\n",
    "\n",
    "**Step 1 — Verify GPU**\n",
    "```python\n",
    "# Check that Colab gave you a GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "````\n",
    "\n",
    "\n",
    "**Step 2 — Install libraries (install **transformers** first)**\n",
    "\n",
    "```python\n",
    "# Install/upgrade essential libraries for HF models on GPU\n",
    "!pip -q install --upgrade transformers accelerate safetensors\n",
    "```\n",
    "\n",
    "**Step 3 — (Optional) Mount Google Drive**\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Skip if you don't need Drive\n",
    "```\n",
    "\n",
    "**Step 4 — Load Tokenizer & Model on GPU (GPT-2)**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"            # If you hit memory issues, try: \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Some GPT-2 variants do not have a pad token; align it with EOS for safety\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Choose dtype: fp16 on GPU saves VRAM, fp32 on CPU\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"Loaded {model_name} on {device} with dtype {dtype}.\")\n",
    "```\n",
    "\n",
    "**Step 5 — Generate Text (your “indian cricket” example)**\n",
    "\n",
    "```python\n",
    "prompt = \"indian cricket\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,               # keep modest for speed/VRAM\n",
    "        do_sample=True,              # sampling makes output more natural\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "```\n",
    "\n",
    ">  If you want to re-run the exact same thing again (like your original snippet did twice),\n",
    "> just **re-run this cell** to generate a fresh sample.\n",
    "\n",
    "**Step 6 — If You Still Hit Memory Errors (OOM)**\n",
    "\n",
    "```python\n",
    "# Try a smaller model first:\n",
    "#   model_name = \"distilgpt2\"\n",
    "\n",
    "# Or let Transformers shard layers across devices automatically (needs accelerate):\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2-medium\"     # Larger than gpt2; may still OOM on T4; try cautiously\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",          # places layers on GPU/CPU as needed\n",
    "    torch_dtype=torch.float16   # reduce VRAM\n",
    ")\n",
    "\n",
    "prompt = \"indian cricket\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "**Other tips**\n",
    "\n",
    "* Reduce `max_length`\n",
    "* Use `gpt2` → `distilgpt2` (smaller)\n",
    "* Clear VRAM: restart runtime if memory gets fragmented\n",
    "\n",
    "\n",
    "\n",
    "# **(Optional) LLM **Chunking** Demo (when input text is long)**\n",
    "\n",
    "> Split long text into chunks so it fits in the model’s context window.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"  # or \"distilgpt2\" if you need lighter model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype).to(device)\n",
    "\n",
    "def chunk_text(text, max_tokens=256):\n",
    "    \"\"\"Split text into token chunks that fit max_tokens.\"\"\"\n",
    "    ids = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "    chunks = [ids[i:i+max_tokens] for i in range(0, len(ids), max_tokens)]\n",
    "    return chunks\n",
    "\n",
    "def generate_for_chunks(chunks, gen_len=80):\n",
    "    outputs = []\n",
    "    with torch.inference_mode():\n",
    "        for ch in chunks:\n",
    "            ch = ch.unsqueeze(0).to(device)\n",
    "            out = model.generate(\n",
    "                ch,\n",
    "                max_length=min(ch.shape[-1] + gen_len, 1024),  # GPT-2 max length\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.8,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            outputs.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    return outputs\n",
    "\n",
    "long_text = \"Artificial Intelligence and NLP are growing rapidly in healthcare, finance, and education. \" * 30\n",
    "chunks = chunk_text(long_text, max_tokens=256)\n",
    "responses = generate_for_chunks(chunks, gen_len=60)\n",
    "\n",
    "for i, r in enumerate(responses, 1):\n",
    "    print(f\"\\n--- Response for chunk {i} ---\\n{r}\\n\")\n",
    "```\n",
    "\n",
    "**Why the same code often fails on CPU**\n",
    "\n",
    "* Large models + long sequences → **slow** or **out-of-memory** on CPU\n",
    "* Missing tokenizer import/usage (you must tokenize before `.generate`)\n",
    "* No `pad_token_id` (some GPT-2 configs need it for generation)\n",
    "\n",
    "**CPU fallback:** use `distilgpt2`, **shorter `max_length`**, and keep inputs small.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "1. Set **Colab → GPU**\n",
    "2. Install **transformers** (first), **accelerate**, **safetensors**\n",
    "3. Verify GPU with **`!nvidia-smi`**\n",
    "4. Load **tokenizer + model** to **GPU** (use fp16)\n",
    "5. Generate text (your **“indian cricket”** prompt)\n",
    "6. If OOM → **smaller model**, **shorter sequences**, or **device\\_map=\"auto\"**\n",
    "\n",
    "> You can now copy each block into **separate Colab cells** and run top to bottom.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0385f03-770a-4aa0-af97-a74a9cc92c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
