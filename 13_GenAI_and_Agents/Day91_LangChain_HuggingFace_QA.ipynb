{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f009c6-239e-4287-bcb3-bae61764f3a9",
   "metadata": {},
   "source": [
    "\n",
    "# LangChain – Q&A with Memory (Updated for HuggingFaceEndpoint)\n",
    "\n",
    "This notebook will guide you from **understanding LangChain** to **building a simple AI assistant** that can **remember past questions**.  \n",
    "\n",
    "\n",
    "> **Note:** LangChain is **actively developed**. Libraries, methods, and classes may change frequently. If you encounter errors, always **check the official LangChain documentation:** https://www.langchain.com/docs/\n",
    " and make sure your packages are up-to-date.\n",
    "\n",
    "# What is LangChain?\n",
    "\n",
    "LangChain is like a **toolkit to build smart AI assistants**.  \n",
    "\n",
    "It combines:\n",
    "\n",
    "- **Brain (LLM)** – The AI that generates answers  \n",
    "- **Prompts** – Instructions you give to the AI  \n",
    "- **Chains** – Step-by-step workflows  \n",
    "- **Agents** – Smart decision-makers that can use tools  \n",
    "- **Memory** – Allows the AI to remember past conversations  \n",
    "\n",
    "> Analogy: Think of LangChain as **your AI friend** with a **brain, notepad, and access to tools**.  \n",
    "> Tip: LangChain frequently updates APIs and methods. If something doesn’t work, check the official docs for the latest method or parameter names.\n",
    "\n",
    "# Key Components\n",
    "\n",
    "1. **LLM (Large Language Model)** – Brain that understands and generates text  \n",
    "2. **PromptTemplate** – Template for asking questions clearly  \n",
    "3. **Chain** – Series of steps to answer questions  \n",
    "4. **Agent** – Chooses the right tool for the task  \n",
    "5. **Memory** – Remembers conversation history  \n",
    "\n",
    "\n",
    "# Installation\n",
    "\n",
    "Run this in a notebook cell:\n",
    "\n",
    "```python\n",
    "!pip install langchain transformers langchain-huggingface\n",
    "```\n",
    "\n",
    "\n",
    "# Get HuggingFace API Token\n",
    "\n",
    "1. Go to [HuggingFace](https://huggingface.co/) and log in or create an account  \n",
    "2. Go to **Settings → Access Tokens → New token**  \n",
    "3. Copy your token  \n",
    "4. Set it in Python:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_TOKEN_HERE\"\n",
    "```\n",
    "\n",
    "Replace `\"YOUR_TOKEN_HERE\"` with your actual token.\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "```python\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "```\n",
    "> Note: If you get an import error, check LangChain docs—the import paths may have changed in updates.\n",
    "\n",
    "# Load the Language Model (Brain)\n",
    "\n",
    "```python\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\":0, \"max_length\":256}\n",
    ")\n",
    "```\n",
    "\n",
    "> This model is our **smart friend** who can answer questions in simple words.\n",
    "\n",
    "\n",
    "\n",
    "#  Create a Prompt Template\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question in simple words: {question}\"\n",
    ")\n",
    "```\n",
    "\n",
    "> This is like writing **clear instructions** on a sticky note for our AI friend.\n",
    "\n",
    "\n",
    "# Add Memory\n",
    "\n",
    "```python\n",
    "memory = ConversationBufferMemory()\n",
    "```\n",
    "\n",
    "> Memory acts like a **notepad** where the AI remembers everything you ask.\n",
    "\n",
    "\n",
    "# Create the Q&A Chain with Memory\n",
    "\n",
    "```python\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt\n",
    ")\n",
    "```\n",
    "\n",
    "> Now the AI can **answer questions and remember previous ones**.\n",
    "\n",
    "\n",
    "# Ask Questions\n",
    "\n",
    "```python\n",
    "# First question\n",
    "answer1 = conversation.run(\"What is LangChain?\")\n",
    "print(\"AI:\", answer1)\n",
    "\n",
    "# Second question\n",
    "answer2 = conversation.run(\"Can you explain its key components?\")\n",
    "print(\"AI:\", answer2)\n",
    "\n",
    "# Third question referring to previous context\n",
    "answer3 = conversation.run(\"Which part is like the brain?\")\n",
    "print(\"AI:\", answer3)\n",
    "```\n",
    "\n",
    "> Notice how the AI **uses memory** to answer related questions.\n",
    "\n",
    "\n",
    "# See the Conversation History\n",
    "\n",
    "```python\n",
    "print(\"Conversation History:\\n\")\n",
    "print(memory.buffer)\n",
    "```\n",
    "\n",
    "> You will see all your previous questions and AI answers, just like a chat history.\n",
    "\n",
    "\n",
    "# Next Steps / Ideas\n",
    "\n",
    "* Explore **Agents** to let AI automatically choose tools  \n",
    "* Connect **APIs** (e.g., weather, calculator, Wikipedia)  \n",
    "* Use **custom prompts** for different answer styles (simple, detailed, humorous)  \n",
    "* Add **more advanced memory** to remember user preferences over long chats  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
