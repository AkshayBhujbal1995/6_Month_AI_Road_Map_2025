{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40828a75-2e41-42e6-b5c0-ceb4df6ee97b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size:28px; font-weight:bold;\">\n",
    "Gen AI: Key Concepts and Foundations\n",
    "</div>\n",
    "\n",
    "#  Large Language Models (LLMs)\n",
    "\n",
    "## What are Large Language Models?\n",
    "    \n",
    "Large Language Models (LLMs) are advanced AI models trained on massive amounts of text data.  \n",
    "They learn the patterns, grammar, facts, and reasoning structures of language to **predict the next word (token)** in a sequence.  \n",
    "\n",
    "We call them **“Large”** because:\n",
    "\n",
    "- They have **billions or even trillions of parameters** (weights in the neural network).\n",
    "- They are trained on **huge datasets** (books, articles, websites, code, etc.).\n",
    "\n",
    "**In simple words:**\n",
    "                                                          \n",
    "LLMs are like super-advanced autocomplete systems that can write essays, answer questions, generate code, and even carry on human-like conversations.\n",
    "\n",
    "\n",
    "## Why do we call them “LLMs”?\n",
    "\n",
    "- **Large** → because of their size (data + parameters).  \n",
    "- **Language** → because their primary focus is understanding and generating natural language.  \n",
    "- **Model** → because they are machine learning models built using deep learning techniques (mainly Transformers).\n",
    "\n",
    "\n",
    "## Use Cases of LLMs\n",
    "- **Conversational AI** → ChatGPT, Gemini, Claude.  \n",
    "- **Content Creation** → blog writing, story generation, marketing content.  \n",
    "- **Programming** → GitHub Copilot, code explanation, debugging.  \n",
    "- **Customer Support** → automated chatbots, FAQ assistants.  \n",
    "- **Education** → tutoring, summarizing study material.  \n",
    "- **Healthcare** → summarizing patient reports, clinical trial search.  \n",
    "- **Enterprise** → analyzing documents, generating reports, summarizing legal contracts.  \n",
    "\n",
    "\n",
    "\n",
    "## Advantages of LLMs\n",
    "    \n",
    "- Can generate **human-like text** across domains.  \n",
    "- **Versatile** → one model can be used for translation, summarization, Q&A, etc.  \n",
    "- Enable **rapid prototyping** of AI applications (chatbots, agents, tools).  \n",
    "- Can integrate with external tools (search engines, databases).  \n",
    "\n",
    "\n",
    "## Limitations of LLMs\n",
    "    \n",
    "- **Hallucinations** → Sometimes they generate **false but confident-sounding answers**.  \n",
    "- **Biases** → They may replicate social, cultural, or gender biases present in training data.  \n",
    "- **High Cost** → Training large models costs millions of dollars in compute and energy.  \n",
    "- **Limited Knowledge** → Models only know up to their training cut-off date unless connected to external data (RAG).  \n",
    "- **Context window constraints** → They cannot remember beyond their max input size.  \n",
    "\n",
    "\n",
    "## Famous LLMs\n",
    "    \n",
    "- **GPT (OpenAI)** → GPT-3, GPT-4, GPT-4o (multimodal).  \n",
    "- **Gemini (Google DeepMind)** → Gemini 1.5, Gemini 2.5.  \n",
    "- **Claude (Anthropic)** → Claude 2, Claude 3.  \n",
    "- **LLaMA (Meta)** → LLaMA 2, LLaMA 3.  \n",
    "- **Mistral AI** → Mistral 7B, Mixtral 8x7B.  \n",
    "- **Cohere** → Command R series.  \n",
    "- **Falcon (TII UAE)** → Falcon 40B, Falcon 180B.  \n",
    "\n",
    "\n",
    "\n",
    "#  How Do LLMs Work?\n",
    "\n",
    "1. **Autoregressive Prediction**  \n",
    "   - LLMs generate text one **token at a time**.  \n",
    "   - Each token generated is fed back as input to predict the next token.  \n",
    "\n",
    "   Example:  \n",
    "   Prompt → \"The cat is on the\"  \n",
    "   Model predicts → \"mat\" (one token at a time).  \n",
    "\n",
    "2. **Transformer Architecture**  \n",
    "   - LLMs use the **Transformer** neural network.  \n",
    "   - Key components:  \n",
    "     - **Self-Attention** → lets the model focus on important words in the input.  \n",
    "     - **Feedforward Layers** → process and transform information.  \n",
    "     - **Positional Encoding** → helps the model understand word order.  \n",
    "\n",
    "3. **Training Process**  \n",
    "   - Trained on **huge datasets** using GPUs/TPUs.  \n",
    "   - Objective: **Minimize the difference** between predicted tokens and actual tokens.  \n",
    "   - Requires **massive compute power, storage, and cost**.  \n",
    "\n",
    "\n",
    "\n",
    "#  Context Window, Temperature, Top-k, and Top-p\n",
    "\n",
    "## Context Window\n",
    "\n",
    "- The **number of tokens (words/pieces of words)** the model can consider at once.  \n",
    "- Larger context window = model can handle **longer conversations/documents**.  \n",
    "- Example: GPT-4 → 128k tokens (about 300 pages of text).  \n",
    "\n",
    "\n",
    "\n",
    "## Temperature\n",
    "- Controls **randomness vs. determinism** in generation.  \n",
    "- Low temperature (0.2) → precise, factual, repetitive answers.  \n",
    "- High temperature (0.8–1.0) → creative, diverse, sometimes less factual.  \n",
    "\n",
    "\n",
    "\n",
    "## Top-k Sampling\n",
    "- The model looks at the **top k most likely next tokens** and randomly picks among them.  \n",
    "- Smaller k → less variety, more focus.  \n",
    "- Larger k → more creativity.  \n",
    "\n",
    "\n",
    "\n",
    "## Top-p (Nucleus Sampling)\n",
    "- Instead of fixed k, it chooses tokens from the **smallest set whose cumulative probability exceeds p**.  \n",
    "- Example: p = 0.9 → tokens chosen from the top 90% probability mass.  \n",
    "- More adaptive than top-k.  \n",
    "\n",
    "\n",
    "\n",
    "## Combined Effect\n",
    "- Developers often **combine Temperature + Top-k + Top-p** to fine-tune outputs.  \n",
    "- Example:  \n",
    "  - **Low temperature + small top-k** → good for factual Q&A.  \n",
    "  - **High temperature + nucleus sampling** → good for storytelling/creative writing.  \n",
    "\n",
    "\n",
    "\n",
    "#  Challenges in LLMs\n",
    "\n",
    "## Hallucinations\n",
    "- LLMs sometimes **make up facts** that sound realistic.  \n",
    "- Example: \"Einstein won the Nobel Prize in Chemistry\" → ❌ incorrect.  \n",
    "- Fix → Use **RAG (Retrieval-Augmented Generation)** to ground answers.  \n",
    "\n",
    "\n",
    "\n",
    "## Security Risks\n",
    "- **Prompt Injection** → malicious inputs can trick LLMs into leaking sensitive info.  \n",
    "- **Data Privacy** → input data may be stored and misused if not handled carefully.  \n",
    "- **Biases** → harmful outputs reflecting bias in training data.  \n",
    "\n",
    "\n",
    "\n",
    "## Cost\n",
    "- Training → requires **massive hardware clusters**.  \n",
    "- Inference → serving billions of requests is expensive (energy + compute).  \n",
    "\n",
    "\n",
    "\n",
    "#  What is a Vector Database?\n",
    "\n",
    "## Definition\n",
    "A **Vector Database** is a specialized database designed to store and search **vector embeddings** (numerical representations of text, images, audio, etc.).\n",
    "\n",
    "\n",
    "## How it Works\n",
    "1. **Embedding Generation**  \n",
    "   - Text, image, or audio is converted into a high-dimensional **vector** (e.g., 768 or 1024 dimensions).  \n",
    "   - Example: \"Cat\" → [0.23, -0.11, 0.98, ...]  \n",
    "\n",
    "2. **Storage**  \n",
    "   - These vectors are stored in a database.  \n",
    "\n",
    "3. **Similarity Search**  \n",
    "   - When a query comes in, it’s converted into a vector.  \n",
    "   - The database finds **nearest neighbors** using Approximate Nearest Neighbor (ANN) search.  \n",
    "   - Example: Query = \"puppy\" → closest match = \"dog\" embeddings.  \n",
    "\n",
    "\n",
    "\n",
    "## Examples of Vector Databases\n",
    "- **Pinecone**  \n",
    "- **Weaviate**  \n",
    "- **Milvus**  \n",
    "- **Qdrant**  \n",
    "- **ChromaDB**  \n",
    "\n",
    "\n",
    "## Why Important in Gen AI?\n",
    "- LLMs don’t **store all knowledge** inside them.  \n",
    "- Vector DBs allow LLMs to **retrieve knowledge dynamically** → enabling **search, RAG, personalization**.  \n",
    "\n",
    "\n",
    "\n",
    "#  What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "## Definition\n",
    "**RAG = Retrieval + Generation**  \n",
    "It combines **retrieving external documents** with **LLM text generation**.  \n",
    "\n",
    "\n",
    "\n",
    "## How it Works\n",
    "1. User asks a question → \"What are the side effects of aspirin?\"  \n",
    "2. System retrieves relevant documents from a **vector database**.  \n",
    "3. The retrieved content is added to the LLM prompt.  \n",
    "4. LLM generates an answer grounded in facts.  \n",
    "\n",
    "\n",
    "\n",
    "## Benefits of RAG\n",
    "- **Reduces hallucinations** by grounding answers in external facts.  \n",
    "- **Keeps models up to date** without retraining.  \n",
    "- **Domain-specific knowledge** → e.g., company policies, medical reports.  \n",
    "\n",
    "\n",
    "\n",
    "## Real-World Applications\n",
    "- **Chatbots** that access company knowledge base.  \n",
    "- **Healthcare assistants** retrieving clinical data.  \n",
    "- **Legal research** retrieving past case law.  \n",
    "- **Education** summarizing textbooks with references.  \n",
    "\n",
    "\n",
    "\n",
    "# Gen AI Application Development Steps\n",
    "\n",
    "1. **Evaluate** → Is this really a Gen AI use case?  \n",
    "2. **Data Collection & Preparation** → Get domain-specific data.  \n",
    "3. **Choose Model Architecture** → GPT, Gemini, Claude, etc.  \n",
    "4. **Model Training / Fine-Tuning** → Adjust for your domain.  \n",
    "5. **Evaluation** → Check accuracy, reliability, bias.  \n",
    "6. **Optimize & Deploy** → Efficient APIs, scalable infra.  \n",
    "7. **Compliance & Ethics** → Ensure data privacy & fairness.  \n",
    "8. **Monitoring & Feedback** → Continuous improvement.  \n",
    "\n",
    "\n",
    "#  Gen AI Tech Stack\n",
    "\n",
    "## Frameworks\n",
    "- LangChain, LlamaIndex, HuggingFace, PyTorch, TensorFlow  \n",
    "\n",
    "## Vector Databases\n",
    "- Pinecone, Qdrant, ChromaDB, Milvus, Weaviate  \n",
    "\n",
    "## Cloud Platforms\n",
    "- AWS Bedrock, Azure OpenAI Service, Google AI Studio  \n",
    "\n",
    "## Famous LLMs\n",
    "- GPT (OpenAI), Gemini (Google), Claude (Anthropic), LLaMA (Meta), Mistral AI  \n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "- LLMs are powerful **but not perfect** → they need external tools like **Vector DBs and RAG**.  \n",
    "- **Prompt engineering** and **parameter tuning (temperature, top-p, top-k)** control model behavior.  \n",
    "- **Vector databases** enable efficient similarity search for unstructured data.  \n",
    "- **RAG** grounds LLMs in facts, making them more reliable.  \n",
    "- Gen AI development is **not just about models**, but also about **data, infra, ethics, and user needs**.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
