{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8bc7c8-f2b9-445e-b9db-952915c89c63",
   "metadata": {},
   "source": [
    "\n",
    "# **spaCy for NLP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d1af9-2ccc-49c2-8b7f-8cd47bc23566",
   "metadata": {},
   "source": [
    "**Introduction to spaCy**\n",
    "\n",
    "- spaCy is a free, open-source Python library for advanced Natural Language Processing (NLP).\n",
    "\n",
    "- It is not an API — meaning it won’t answer questions out of the box like ChatGPT, but it provides powerful tools to process and analyze text.\n",
    "\n",
    "- Key idea: Whatever we did using NLTK, we can also do (and often faster) with spaCy.\n",
    "\n",
    "- spaCy is designed for production use (speed + efficiency) and supports pre-trained language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf676cd-0c96-4a25-9420-28fa27fce076",
   "metadata": {},
   "source": [
    "# Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08458e0c-7f9b-439f-b767-49920d400c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy (only once)\n",
    "!pip install spacy\n",
    "\n",
    "# Download the English language model (small)\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For larger models:\n",
    "# en_core_web_md  (medium)\n",
    "# en_core_web_lg  (large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04121afb-c081-456b-a7a8-b04757b60293",
   "metadata": {},
   "source": [
    "# Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60412e3-7244-45ee-888b-49ea6d94eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Process the text into a spaCy Doc object\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60862cf-af40-4c53-b610-f84f1e410a29",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2fe7c16-c13c-421e-b1f3-4453f01e8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "# Print individual tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e089efe-cb3d-4c21-a933-7e6887cf9c0c",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681f7ab3-4dfa-49fb-8c4e-40143fc040e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN\n",
      "is : AUX\n",
      "looking : VERB\n",
      "at : ADP\n",
      "buying : VERB\n",
      "U.K. : PROPN\n",
      "startup : VERB\n",
      "for : ADP\n",
      "$ : SYM\n",
      "1 : NUM\n",
      "billion : NUM\n"
     ]
    }
   ],
   "source": [
    "# Print tokens with their POS tags\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db0995-0951-4b50-aba2-3349c28d4f3e",
   "metadata": {},
   "source": [
    "## Lemmatization & Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af494e4-cf7b-4eed-bd89-7a23b6081d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN --> Apple | Dependency: nsubj\n",
      "is : AUX --> be | Dependency: aux\n",
      "looking : VERB --> look | Dependency: ROOT\n",
      "at : ADP --> at | Dependency: prep\n",
      "buying : VERB --> buy | Dependency: pcomp\n",
      "U.K. : PROPN --> U.K. | Dependency: nsubj\n",
      "startup : VERB --> startup | Dependency: ccomp\n",
      "for : ADP --> for | Dependency: prep\n",
      "$ : SYM --> $ | Dependency: quantmod\n",
      "1 : NUM --> 1 | Dependency: compound\n",
      "billion : NUM --> billion | Dependency: pobj\n"
     ]
    }
   ],
   "source": [
    "# Tokens with POS, Lemma (base form), and Dependency relation\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"| Dependency:\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45b72a-41d8-4233-90f4-8eec2a6fea51",
   "metadata": {},
   "source": [
    "## Extended Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14102c17-b14e-4067-9913-1fd27cca9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN --> Apple | Dep: nsubj | Shape: Xxxxx | Alpha: True | Stopword: False\n",
      "is : AUX --> be | Dep: aux | Shape: xx | Alpha: True | Stopword: True\n",
      "looking : VERB --> look | Dep: ROOT | Shape: xxxx | Alpha: True | Stopword: False\n",
      "at : ADP --> at | Dep: prep | Shape: xx | Alpha: True | Stopword: True\n",
      "buying : VERB --> buy | Dep: pcomp | Shape: xxxx | Alpha: True | Stopword: False\n",
      "U.K. : PROPN --> U.K. | Dep: nsubj | Shape: X.X. | Alpha: False | Stopword: False\n",
      "startup : VERB --> startup | Dep: ccomp | Shape: xxxx | Alpha: True | Stopword: False\n",
      "for : ADP --> for | Dep: prep | Shape: xxx | Alpha: True | Stopword: True\n",
      "$ : SYM --> $ | Dep: quantmod | Shape: $ | Alpha: False | Stopword: False\n",
      "1 : NUM --> 1 | Dep: compound | Shape: d | Alpha: False | Stopword: False\n",
      "billion : NUM --> billion | Dep: pobj | Shape: xxxx | Alpha: True | Stopword: False\n"
     ]
    }
   ],
   "source": [
    "# All in one: POS, Lemma, Dependency, Shape, Alphabet check, Stop word check\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"|\", \n",
    "          \"Dep:\", token.dep_, \"| Shape:\", token.shape_, \n",
    "          \"| Alpha:\", token.is_alpha, \"| Stopword:\", token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c8fcd-aca0-472e-8687-489692cf0b58",
   "metadata": {},
   "source": [
    "> Note — **spaCy does not do *everything* NLTK does**, but it covers almost all the **practical / production-level NLP tasks**. Let me break this down clearly for your notebook:\n",
    "\n",
    "# NLTK vs spaCy – Coverage\n",
    "\n",
    "## Things you can do with both NLTK & spaCy\n",
    "\n",
    "* **Tokenization** (word, sentence, paragraph)\n",
    "* **Part of Speech (POS) tagging**\n",
    "* **Lemmatization**\n",
    "* **Dependency Parsing**\n",
    "* **Named Entity Recognition (NER)**\n",
    "* **Stopword Removal**\n",
    "* **Word Similarity (using embeddings)**\n",
    "* **Text Classification (with training)**\n",
    "\n",
    "## Things spaCy does better than NLTK\n",
    "\n",
    "* **Speed** → much faster for large text.\n",
    "* **Pre-trained models** → spaCy has `en_core_web_sm`, `md`, `lg`.\n",
    "* **NER & Dependency Parsing** → built-in and more accurate.\n",
    "* **Integration with deep learning** → easily works with PyTorch, TensorFlow.\n",
    "* **Pipeline structure** → everything runs in a single `nlp()` call.\n",
    "\n",
    "## Things NLTK does that spaCy does not (or less focused)\n",
    "\n",
    "* **Corpora access** (movie reviews, Brown corpus, WordNet, etc.).\n",
    "* **Linguistic resources** (CFG parsing, grammar trees, etc.).\n",
    "* **Educational focus** (helps students learn concepts).\n",
    "* **Rule-based text processing** (regex tokenizer, stemmers).\n",
    "* **Language coverage** → NLTK supports more small academic datasets.\n",
    "\n",
    "\n",
    "## Key Points:\n",
    "    \n",
    "* **If you want research, corpora, and learning basics → use NLTK.**\n",
    "* **If you want production-ready pipelines, speed, and accuracy → use spaCy.**\n",
    "* In real projects → many people **learn with NLTK, but deploy with spaCy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3530b-bcf5-4e3f-922b-3a8aed87381e",
   "metadata": {},
   "source": [
    "## Quick Example: Same task in NLTK vs spaCy\n",
    "\n",
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618ef189-803e-4f85-a712-ef3298eddfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4001be-7389-448e-bff1-b898a4197aef",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a25e9102-46c2-47bf-8467-210b8526b98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03f348-447b-4268-a462-f8b2cda30898",
   "metadata": {},
   "source": [
    "## **Advantages of spaCy**\n",
    "\n",
    "- Fast & efficient for large-scale NLP tasks.\n",
    "\n",
    "- Provides state-of-the-art models for POS tagging, NER, dependency parsing.\n",
    "\n",
    "- Easy to integrate with deep learning frameworks (TensorFlow, PyTorch).\n",
    "\n",
    "- Production-ready with optimized pipelines.\n",
    "\n",
    "## **Disadvantages of spaCy**\n",
    "\n",
    "- Less suitable for linguistic research (NLTK has more linguistic corpora).\n",
    "\n",
    "- Requires downloading large models for advanced features.\n",
    "\n",
    "- Fewer built-in datasets compared to NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcf1c9-c2f7-4933-8d58-ebb8d4a6bda1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- spaCy is a modern alternative to NLTK, better suited for real-world applications.\n",
    "\n",
    "- It makes text processing pipelines faster and simpler.\n",
    "\n",
    "- Great for tasks like Tokenization, POS tagging, Lemmatization, Named Entity Recognition (NER), and Dependency Parsing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
